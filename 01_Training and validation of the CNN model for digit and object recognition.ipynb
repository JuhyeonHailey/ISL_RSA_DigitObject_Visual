{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Basically, referred to https://github.com/sjchoi86/advanced-tensorflow/blob/master/basic/cnn_mnist_modern.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tensorflow as tf\n",
    "from imgaug import augmenters as iaa\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pre-trained weights\n",
    "with open('vgg16_weights.txt','rb') as fp: # tf.keras.applications.VGG16()\n",
    "    pw = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vgg16\n",
    "# https://github.com/tensorflow/models/blob/master/research/slim/nets/vgg.py\n",
    "def vgg_arg_scope(weight_decay=0.0005):\n",
    "    \"\"\"Defines the VGG arg scope.\n",
    "    Args:\n",
    "    weight_decay: The l2 regularization coefficient.\n",
    "    Returns:\n",
    "    An arg_scope.\n",
    "    \"\"\"\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                  activation_fn=tf.nn.relu,\n",
    "                  weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                  biases_initializer=tf.compat.v1.zeros_initializer()):\n",
    "        with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:\n",
    "            return arg_sc\n",
    "        \n",
    "n_input = 196608 # 224*224*3\n",
    "n_classes = 16\n",
    "\n",
    "x = tf.placeholder(\"float\", [None,224,224,3])\n",
    "y = tf.placeholder(\"float\", [None, n_classes]) \n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def vgg16(inputs, is_training=True, dropout_keep_prob=0.5):\n",
    "    with slim.arg_scope(vgg_arg_scope()):\n",
    "        with tf.variable_scope('conv1'):\n",
    "            net = slim.conv2d(inputs, 64, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[0]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[1]),\n",
    "                              trainable=True,\n",
    "                              scope='conv1_1')\n",
    "            net = slim.conv2d(net, 64, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[2]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[3]),\n",
    "                              trainable=True,\n",
    "                              scope='conv1_2')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool1') # 112,112,64\n",
    "        with tf.variable_scope('conv2'):\n",
    "            net = slim.conv2d(net, 128, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[4]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[5]),\n",
    "                              trainable=True,\n",
    "                              scope='conv2_1')\n",
    "            net = slim.conv2d(net, 128, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[6]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[7]),\n",
    "                              trainable=True,\n",
    "                              scope='conv2_2')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool2') # 56,56,128\n",
    "        with tf.variable_scope('conv3'):\n",
    "            net = slim.conv2d(net, 256, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[8]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[9]),\n",
    "                              trainable=True,\n",
    "                              scope='conv3_1')\n",
    "            net = slim.conv2d(net, 256, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[10]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[11]),\n",
    "                              trainable=True,\n",
    "                              scope='conv3_2')\n",
    "            net = slim.conv2d(net, 256, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[12]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[13]),\n",
    "                              trainable=True,\n",
    "                              scope='conv3_3')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool3') # 28,28,256\n",
    "        with tf.variable_scope('conv4'):\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[14]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[15]),\n",
    "                              trainable=True,\n",
    "                              scope='conv4_1')\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[16]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[17]),\n",
    "                              trainable=True,\n",
    "                              scope='conv4_2')\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[18]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[19]),\n",
    "                              trainable=True,\n",
    "                              scope='conv4_3')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool4') # 14x14x512\n",
    "        with tf.variable_scope('conv5'):\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[20]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[21]),\n",
    "                              trainable=True,\n",
    "                              scope='conv5_1')\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[22]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[23]),\n",
    "                              trainable=True,\n",
    "                              scope='conv5_2')\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[24]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[25]),\n",
    "                              trainable=True,\n",
    "                              scope='conv5_3')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool5') # 7,7,512\n",
    "        \n",
    "        # fc\n",
    "        net = slim.conv2d(net, 1024, [7, 7], padding='VALID', scope='fc6') # cf.4096\n",
    "        net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout6')\n",
    "        # output\n",
    "        net = slim.conv2d(net, n_classes, [1, 1], activation_fn=None, normalizer_fn=None,\n",
    "                          scope='fc8')\n",
    "        # spatial squeeze\n",
    "        out = tf.squeeze(net, name ='SpatialSqueeze')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred = vgg16(x, is_training)\n",
    "\n",
    "# Loss & Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
    "\n",
    "# With decay\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.00001\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           8050, 0.96, staircase=True)\n",
    "optm = tf.train.AdamOptimizer(learning_rate).minimize(cost,global_step=global_step) \n",
    "\n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "\n",
    "# Initializer\n",
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "print (\"FUNCTIONS READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images\n",
    "- Load training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = np.load('Train_Val.npz')\n",
    "mnst_tr_X, mnst_tr_y, mnst_val_X, mnst_val_y = l['mnst_tr_X'], l['mnst_tr_y'], l['mnst_val_X'], l['mnst_val_y']\n",
    "imgnet_tr_X, imgnet_tr_y, imgnet_val_X, imgnet_val_y = l['imgnet_tr_X'], l['imgnet_tr_y'], l['imgnet_val_X'], l['imgnet_val_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training set preprocessing and augmentation\n",
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((1,1,3))\n",
    "ntrain_mnst = mnst_tr_X.shape[0]\n",
    "ntrain_imgnet = imgnet_tr_X.shape[0]\n",
    "\n",
    "# mnst\n",
    "def aug_mnst(xs_):\n",
    "    xs_ = xs_.reshape(ntrain_mnst,224,224,3).copy().astype(np.float32)\n",
    "    mnst_aug = iaa.Affine(scale=(1.0, 1.08), translate_percent=(-0.08, 0.08), rotate=(-15, 15))\n",
    "    xs_ = mnst_aug.augment_images(xs_).astype(np.float32)\n",
    "    xs2 = np.zeros((ntrain_mnst,224,224,3),dtype=np.float32)\n",
    "    for i_, x_ in enumerate(xs_):\n",
    "        x_ = x_ - vgg_mean\n",
    "        xs2[i_] = x_[:,:, ::-1]\n",
    "    return xs2\n",
    "\n",
    "# imgnet\n",
    "c9 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='left-top'),iaa.Fliplr(0.5)])\n",
    "c10 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='center-top'),iaa.Fliplr(0.5)])\n",
    "c11 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='right-top'),iaa.Fliplr(0.5)])\n",
    "c12 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='left-center'),iaa.Fliplr(0.5)])\n",
    "c13 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='center'),iaa.Fliplr(0.5)])\n",
    "c14 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='right-center'),iaa.Fliplr(0.5)])\n",
    "c15 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='left-bottom'),iaa.Fliplr(0.5)])\n",
    "c16 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='center-bottom'),iaa.Fliplr(0.5)])\n",
    "c17 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='right-bottom'),iaa.Fliplr(0.5)])\n",
    "c18 = iaa.Sequential([iaa.Resize({\"height\": 224, \"width\": 224}),\n",
    "                      iaa.Fliplr(0.5)])\n",
    "aug = iaa.OneOf([c9, c10, c11, c12, c13, c14, c15, c16, c17,c18])\n",
    "\n",
    "def fancy_pca(img, alpha_std=0.1):\n",
    "    # https://github.com/pixelatedbrian/fortnight-furniture/blob/master/src/fancy_pca.py\n",
    "    orig_img = img.astype('float32').copy()\n",
    "    img = img / 255.0  \n",
    "    img_rs = img.reshape(-1,3)\n",
    "    img_centered = img_rs - np.mean(img_rs, axis=0)\n",
    "    img_cov = np.cov(img_centered, rowvar=False)\n",
    "    eig_vals, eig_vecs = np.linalg.eigh(img_cov)\n",
    "    sort_perm = eig_vals[::-1].argsort()\n",
    "    eig_vals[::-1].sort()\n",
    "    eig_vecs = eig_vecs[:, sort_perm]\n",
    "    m1 = np.column_stack((eig_vecs))\n",
    "    m2 = np.zeros((3, 1))\n",
    "    alpha = np.random.normal(0, alpha_std)\n",
    "    m2[:, 0] = alpha * eig_vals[:]\n",
    "    add_vect = np.matrix(m1) * np.matrix(m2)\n",
    "    for idx in range(3):   # RGB\n",
    "        orig_img[..., idx] += add_vect[idx]\n",
    "    orig_img = np.clip(orig_img, 0.0, 255.0)\n",
    "    return orig_img\n",
    "\n",
    "def aug_imgnet(xs_):\n",
    "    xs_ = xs_.reshape(ntrain_imgnet,256,256,3).copy().astype(np.float32)\n",
    "    xs_ = np.array(aug.augment_images(xs_))\n",
    "    xs2 = np.zeros((ntrain_imgnet,224,224,3),dtype=np.float32)\n",
    "    for i_, x_ in enumerate(xs_):\n",
    "        x_ = fancy_pca(x_) - vgg_mean # fancy_pca & subtract mean\n",
    "        xs2[i_] = x_[:,:,::-1] # to bgr\n",
    "    return xs2\n",
    "\n",
    "# Validation set preprocessing\n",
    "nval_mnst = mnst_val_X.shape[0]\n",
    "nval_imgnet = imgnet_val_X.shape[0]\n",
    "nval = nval_mnst+nval_imgnet\n",
    "val_X_proc = np.zeros((nval,224,224,3),dtype=np.float32)\n",
    "# mnst\n",
    "for i_, img_ in enumerate(mnst_val_X):\n",
    "    new = img_.reshape(224,224,3).astype(np.float32) - vgg_mean\n",
    "    val_X_proc[i_] = new[:,:,::-1]\n",
    "# imgnet\n",
    "for i_, img_ in enumerate(imgnet_val_X):\n",
    "    new = img_.reshape(224,224,3).astype(np.float32) - vgg_mean\n",
    "    val_X_proc[i_+nval_mnst] = new[:,:,::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save\n",
    "savedir = \"/path/to/save/the/model/\" # Please set the save directory\n",
    "saver = tf.train.Saver(max_to_keep=100)\n",
    "save_step = 10 # set save step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = 56 # set epoch number\n",
    "batch_size = 32 # set batch size\n",
    "display_step = 10 # set display step \n",
    "ntrain = ntrain_mnst+ntrain_imgnet\n",
    "total_batch = int(ntrain/batch_size)\n",
    "# Validation\n",
    "val_batch_size = 16 \n",
    "total_batch_val = int(nval/val_batch_size)\n",
    "# y label\n",
    "# trn\n",
    "y_trn = np.zeros((ntrain,16),dtype='float32')\n",
    "y_trn[:ntrain_mnst] = mnst_tr_y\n",
    "y_trn[ntrain_mnst:] = imgnet_tr_y\n",
    "# val\n",
    "y_val = np.zeros((nval,16),dtype='float32')\n",
    "y_val[:nval_mnst] = mnst_val_y\n",
    "y_val[nval_mnst:] = imgnet_val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimize\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost, avg_acc = 0., 0.\n",
    "    val_cost, val_acc = 0., 0.\n",
    "    # We used random undersampling in the analysis.\n",
    "    # Augmentation\n",
    "    x_trn = np.zeros((ntrain,224,224,3),dtype='float32')\n",
    "    x_trn[:ntrain_mnst] = aug_mnst(mnst_tr_X)\n",
    "    x_trn[ntrain_mnst:] = aug_imgnet(imgnet_tr_X)\n",
    "    randpermlist_m = np.random.permutation(ntrain_mnst)\n",
    "    randpermlist_i = np.random.permutation(range(ntrain_mnst,ntrain))\n",
    "    # Iteration\n",
    "    for i in range(total_batch):\n",
    "        # Balanced mini-batch\n",
    "        randidx_m = randpermlist_m[i*20:min((i+1)*20, ntrain_mnst-1)]\n",
    "        randidx_i = randpermlist_i[i*12:min((i+1)*12, ntrain_imgnet-1)]\n",
    "        randidx = list(randidx_m)+list(randidx_i)\n",
    "        random.shuffle(randidx)\n",
    "        batch_xs = x_trn[randidx, :]\n",
    "        batch_ys = y_trn[randidx, :]\n",
    "        feeds = {x: batch_xs, y: batch_ys, is_training:True}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "    # Acc and cost \n",
    "        avg_cost += sess.run(cost, feed_dict=feeds)\n",
    "        avg_acc += sess.run(accr, feed_dict=feeds)\n",
    "    avg_cost = avg_cost / total_batch\n",
    "    avg_acc = avg_acc / total_batch\n",
    "    print('\\nEpoch :', epoch+1)\n",
    "    print('Train Acc/Cost : %.5f / %.5f'%(avg_acc,avg_cost))\n",
    "    # Display validation performance\n",
    "    if (epoch+1) % display_step == 0:\n",
    "        for i in range(total_batch_val):\n",
    "            batch_xs = val_X_proc[i*val_batch_size:min((i+1)*val_batch_size, nval-1)]\n",
    "            batch_ys = y_val[i*val_batch_size:min((i+1)*val_batch_size, nval-1)]\n",
    "            feeds = {x: batch_xs, y: batch_ys, is_training:False}\n",
    "            val_acc += sess.run(accr, feed_dict=feeds)\n",
    "            val_cost += sess.run(cost, feed_dict=feeds)\n",
    "        val_acc = val_acc / total_batch_val\n",
    "        val_cost = val_cost / total_batch_val\n",
    "        print('Val Acc/Cost : %.5f / %.5f'%(val_acc,val_cost))\n",
    "    # Save\n",
    "    if (epoch+1) % save_step == 0:\n",
    "        savename = savedir + \"net-\" + str(epoch) + \".ckpt\"\n",
    "        saver.save(sess=sess, save_path=savename)\n",
    "        print(\"[%s] Saved\" % (savename))\n",
    "print(\"Optimization finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
