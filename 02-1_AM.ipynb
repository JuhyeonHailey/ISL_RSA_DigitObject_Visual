{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.ndimage as nd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((1,1,3))\n",
    "# Deepdraw\n",
    "# https://github.com/auduno/deepdraw\n",
    "background_color = np.float32([200.0, 200.0, 200.0])\n",
    "gen_image = np.random.normal(background_color, 8, (224, 224, 3))\n",
    "\n",
    "def preprocess(img):\n",
    "    return (img - vgg_mean)[:,:,::-1] \n",
    "def deprocess(img):\n",
    "    return (img + vgg_mean[:,:,::-1])[:,:,::-1] \n",
    "\n",
    "def T(layer):\n",
    "    '''Helper for getting layer output tensor'''\n",
    "    return graph.get_tensor_by_name(layer)\n",
    "\n",
    "def blur(img, sigma):\n",
    "    if sigma > 0:\n",
    "        img[:,:,0] = nd.filters.gaussian_filter(img[:,:,0], sigma, order=0)\n",
    "        img[:,:,1] = nd.filters.gaussian_filter(img[:,:,1], sigma, order=0)\n",
    "        img[:,:,2] = nd.filters.gaussian_filter(img[:,:,2], sigma, order=0)\n",
    "    return img\n",
    "\n",
    "def render_naive_blur_clip(t_obj, img0=gen_image, iter_n=32,norm_param=1e-9,\n",
    "                      start_step_size=1.5,end_step_size=1.5,\n",
    "                      start_sigma=0.5, end_sigma=0.5,\n",
    "                      clip=True):\n",
    "    t_score = tf.reduce_mean(t_obj) # defining the optimization objective\n",
    "    t_grad = tf.gradients(t_score, t_input)[0] # behold the power of automatic differentiation!\n",
    "    \n",
    "    img = preprocess(img0.copy())\n",
    "    img = np.expand_dims(img,0)\n",
    "    w_, h_ = img.shape[1],img.shape[2]\n",
    "    for i in range(iter_n): \n",
    "        g, score = sess.run([t_grad, t_score], {t_input:img, is_training:False})\n",
    "        step = start_step_size + end_step_size - start_step_size * i / iter_n\n",
    "        img += step/np.abs(g).mean()*g\n",
    "        if clip:\n",
    "            bias = vgg_mean\n",
    "            img = np.clip(img, -bias, 255-bias)\n",
    "        sigma = start_sigma + end_sigma - start_sigma * i / iter_n\n",
    "        img[0] = blur(img[0], sigma)\n",
    "    a = deprocess(img[0])\n",
    "    if not clip:\n",
    "        a = a*(255.0/np.percentile(a, 99.98))\n",
    "    a = np.uint8(np.clip(a,0,255))\n",
    "    a = np.reshape(a, [w_, h_,3])\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "\n",
    "t_input = tf.placeholder(tf.float32, name='input') # define the input tensor\n",
    "\n",
    "saver = tf.train.import_meta_graph('net-55.ckpt.meta',input_map={\"Placeholder:0\": t_input})\n",
    "saver.restore(sess, 'net-55.ckpt')\n",
    "is_training = graph.get_tensor_by_name(\"Placeholder_2:0\")\n",
    "\n",
    "layer = 'fc8/Conv2D:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class visualization\n",
    "- visualize-VGGS-filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = [{'iter_n':200,\n",
    "        'start_sigma':2.5,\n",
    "        'end_sigma':1.1,\n",
    "        'start_step_size':12.*0.25,\n",
    "        'end_step_size':10.*0.25,\n",
    "    },\n",
    "    {\n",
    "        'iter_n':100,\n",
    "        'start_sigma':1.1,\n",
    "        'end_sigma':0.78*1.1,\n",
    "        'start_step_size':10.*0.25,\n",
    "        'end_step_size':8.*0.25\n",
    "    },\n",
    "    {\n",
    "        'scale':1.05,\n",
    "        'iter_n':100,\n",
    "        'start_sigma':0.78*1.1,\n",
    "        'end_sigma':0.78,\n",
    "        'start_step_size':8.*0.25,\n",
    "        'end_step_size':6.*0.25\n",
    "    },\n",
    "    {\n",
    "        'scale':1.05,\n",
    "        'iter_n':50,\n",
    "        'start_sigma':0.78*1.1,\n",
    "        'end_sigma':0.40,\n",
    "        'start_step_size':6.*0.25,\n",
    "        'end_step_size':1.5*0.25\n",
    "    },\n",
    "    {\n",
    "        'scale':1.05,\n",
    "        'iter_n':25,\n",
    "        'start_sigma':0.4,\n",
    "        'end_sigma':0.1,\n",
    "        'start_step_size':1.5*0.25,\n",
    "        'end_step_size':0.5*0.25\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "itern_tmp = len(params)\n",
    "f,axes = plt.subplots(4,4, figsize=(14,14))\n",
    "ax = axes.flat\n",
    "for i in range(16):\n",
    "    imgtmp = gen_image.copy()\n",
    "    for itern in range(itern_tmp-1):\n",
    "        imgtmp = render_naive_blur_clip(T(layer)[:,:,:,i],iter_n=params[itern]['iter_n'],\n",
    "                                        img0=imgtmp,start_sigma=params[itern]['start_sigma'],\n",
    "                                        end_sigma=params[itern]['end_sigma'],\n",
    "                                        start_step_size=params[itern]['start_step_size'],\n",
    "                                        end_step_size=params[itern]['end_step_size'])\n",
    "    ax[i].imshow(render_naive_blur_clip(T(layer)[:,:,:,i],iter_n=params[itern_tmp-1]['iter_n'],img0=imgtmp,\n",
    "             start_sigma=params[itern_tmp-1]['start_sigma'],end_sigma=params[itern_tmp-1]['end_sigma'],\n",
    "             start_step_size=params[itern_tmp-1]['start_step_size'],end_step_size=params[itern_tmp-1]['end_step_size']))\n",
    "    ax[i].axis('off')\n",
    "    plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conv filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "convs = [op.name for op in tf.get_default_graph().get_operations() if op.type=='Conv2D']\n",
    "convs = convs[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example deepdraw parameter set 1\n",
    "start_sigma, end_sigma = 0.78, 0.78*0.8\n",
    "start_step_size, end_step_size = 3., 3.\n",
    "iter_n = 200\n",
    "# Plot\n",
    "for li in range(14):\n",
    "    layer = convs[li] +':0'\n",
    "    tmp = T(layer)\n",
    "    nof = int(tmp.shape[-1])\n",
    "    fn_list = random.sample(range(nof),16)\n",
    "    f,axes = plt.subplots(4,4, figsize=(14,14))\n",
    "    ax = axes.flat\n",
    "    for ai in range(16):\n",
    "        fi = fn_list[ai]\n",
    "        ax[ai].set_title('Feature '+str(fi+1),fontsize=15)\n",
    "        ax[ai].imshow(render_naive_blur_clip(T(layer)[:,:,:,fi],\n",
    "                        img0=gen_image,iter_n=iter_n,\n",
    "                     start_sigma=start_sigma,end_sigma=end_sigma,\n",
    "                     start_step_size=start_step_size,end_step_size=end_step_size))\n",
    "        ax[ai].axis('off')\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Example deepdraw parameter set 2\n",
    "for li in range(14):\n",
    "    layer = convs[li] +':0'\n",
    "    tmp = T(layer)\n",
    "    nof = int(tmp.shape[-1])\n",
    "    fn_list = random.sample(range(nof),16)\n",
    "    f,axes = plt.subplots(4,4, figsize=(14,14))\n",
    "    ax = axes.flat\n",
    "    for ai in range(16):\n",
    "        fi = fn_list[ai]\n",
    "        ax[ai].set_title('Feature '+str(fi+1),fontsize=15)\n",
    "        tmp_ = render_naive_blur_clip(T(layer)[:,:,:,fi],iter_n=500,\n",
    "         start_sigma=2.5,end_sigma=0.78,\n",
    "         start_step_size=2.,end_step_size=2.)\n",
    "        ax[ai].imshow(render_naive_blur_clip(T(layer)[:,:,:,fi],iter_n=500,img0=tmp_,\n",
    "         start_sigma=2.5*0.5,end_sigma=0.78*0.5,\n",
    "         start_step_size=1.,end_step_size=1.))\n",
    "        ax[ai].axis('off')\n",
    "        plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
