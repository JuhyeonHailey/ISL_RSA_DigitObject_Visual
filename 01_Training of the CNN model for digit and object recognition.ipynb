{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cdd_05_unfreezing_fc1024_lr1e-5_200805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "from mlxtend.data import loadlocal_mnist\n",
    "from imgaug import augmenters as iaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cdd_n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\" # \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vgg_arg_scope(weight_decay=0.0005):\n",
    "    \"\"\"Defines the VGG arg scope.\n",
    "    Args:\n",
    "    weight_decay: The l2 regularization coefficient.\n",
    "    Returns:\n",
    "    An arg_scope.\n",
    "    \"\"\"\n",
    "    with slim.arg_scope([slim.conv2d, slim.fully_connected],\n",
    "                  activation_fn=tf.nn.relu,\n",
    "                  weights_regularizer=slim.l2_regularizer(weight_decay),\n",
    "                  biases_initializer=tf.compat.v1.zeros_initializer()):\n",
    "        with slim.arg_scope([slim.conv2d], padding='SAME') as arg_sc:\n",
    "            return arg_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load pre-trained weights\n",
    "import pickle\n",
    "with open('vgg16_weights.txt','rb') as fp:\n",
    "    pw = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vgg16 \n",
    "n_input = 196608\n",
    "n_classes = 16\n",
    "\n",
    "x = tf.placeholder(\"float\", [None,224,224,3])\n",
    "y = tf.placeholder(\"float\", [None, n_classes]) \n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "def vgg16(inputs, is_training=True, dropout_keep_prob=0.5):\n",
    "    #init_func = tf.truncated_normal_initializer(stddev=0.01)\n",
    "    with slim.arg_scope(vgg_arg_scope()):\n",
    "        #net = slim.repeat(inputs, 2, slim.conv2d, 64, [3, 3], scope='conv1') # , trainable=False\n",
    "        with tf.variable_scope('conv1'):\n",
    "            net = slim.conv2d(inputs, 64, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[0]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[1]),\n",
    "                              trainable=True,\n",
    "                              scope='conv1_1')\n",
    "            net = slim.conv2d(net, 64, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[2]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[3]),\n",
    "                              trainable=True,\n",
    "                              scope='conv1_2')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool1') # 112,112,64\n",
    "        #net = slim.repeat(net, 2, slim.conv2d, 128, [3, 3], scope='conv2')\n",
    "        with tf.variable_scope('conv2'):\n",
    "            net = slim.conv2d(net, 128, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[4]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[5]),\n",
    "                              trainable=True,\n",
    "                              scope='conv2_1')\n",
    "            net = slim.conv2d(net, 128, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[6]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[7]),\n",
    "                              trainable=True,\n",
    "                              scope='conv2_2')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool2') # 56,56,128\n",
    "        #net = slim.repeat(net, 3, slim.conv2d, 256, [3, 3], scope='conv3')\n",
    "        with tf.variable_scope('conv3'):\n",
    "            net = slim.conv2d(net, 256, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[8]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[9]),\n",
    "                              trainable=True,\n",
    "                              scope='conv3_1')\n",
    "            net = slim.conv2d(net, 256, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[10]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[11]),\n",
    "                              trainable=True,\n",
    "                              scope='conv3_2')\n",
    "            net = slim.conv2d(net, 256, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[12]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[13]),\n",
    "                              trainable=True,\n",
    "                              scope='conv3_3')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool3') # 28,28,256\n",
    "        with tf.variable_scope('conv4'):\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[14]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[15]),\n",
    "                              trainable=True,\n",
    "                              scope='conv4_1')\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[16]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[17]),\n",
    "                              trainable=True,\n",
    "                              scope='conv4_2')\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[18]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[19]),\n",
    "                              trainable=True,\n",
    "                              scope='conv4_3')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool4') # 14x14x512\n",
    "        with tf.variable_scope('conv5'):\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[20]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[21]),\n",
    "                              trainable=True,\n",
    "                              scope='conv5_1')\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[22]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[23]),\n",
    "                              trainable=True,\n",
    "                              scope='conv5_2')\n",
    "            net = slim.conv2d(net, 512, [3, 3], stride=1,\n",
    "                              weights_initializer=tf.constant_initializer(pw[24]),\n",
    "                              biases_initializer=tf.constant_initializer(pw[25]),\n",
    "                              trainable=True,\n",
    "                              scope='conv5_3')\n",
    "        net = slim.max_pool2d(net, [2, 2], scope='pool5') # 7,7,512\n",
    "        \n",
    "        # fc\n",
    "        net = slim.conv2d(net, 1024, [7, 7], padding='VALID', scope='fc6') # cf.4096\n",
    "        net = slim.dropout(net, dropout_keep_prob, is_training=is_training, scope='dropout6')\n",
    "        # output\n",
    "        net = slim.conv2d(net, n_classes, [1, 1], activation_fn=None, normalizer_fn=None,\n",
    "                          scope='fc8')\n",
    "        # spatial squeeze\n",
    "        out = tf.squeeze(net, name ='SpatialSqueeze')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred = vgg16(x, is_training)\n",
    "\n",
    "# Loss & Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=pred))\n",
    "\n",
    "#with decay\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "starter_learning_rate = 0.00001\n",
    "learning_rate = tf.train.exponential_decay(starter_learning_rate, global_step,\n",
    "                                           8050, 0.96, staircase=True)\n",
    "optm = tf.train.AdamOptimizer(learning_rate).minimize(cost,global_step=global_step) \n",
    "\n",
    "corr = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accr = tf.reduce_mean(tf.cast(corr, \"float\"))\n",
    "\n",
    "# Tensorboard\n",
    "with tf.name_scope('Accuarcy'):\n",
    "    accr_ph = tf.placeholder(\"float\", shape=None, name='Acc_summary')\n",
    "    accr_summ = tf.summary.scalar('Acc', accr_ph)\n",
    "    #sep\n",
    "    accr_ph_mnst = tf.placeholder(\"float\", shape=None, name='Acc_mnst_summary')\n",
    "    accr_summ_mnst = tf.summary.scalar('Acc_mnst', accr_ph_mnst)\n",
    "    accr_ph_imgnet = tf.placeholder(\"float\", shape=None, name='Acc_imgnet_summary')\n",
    "    accr_summ_imgnet = tf.summary.scalar('Acc_imgnet', accr_ph_imgnet)\n",
    "with tf.name_scope('Cost'):\n",
    "    loss_ph = tf.placeholder(\"float\", shape=None, name='Cost_summary')\n",
    "    loss_summ = tf.summary.scalar('Cost', loss_ph)\n",
    "    loss_ph_mnst = tf.placeholder(\"float\", shape=None, name='Cost_mnst_summary')\n",
    "    loss_summ_mnst = tf.summary.scalar('Cost_mnst', loss_ph_mnst)\n",
    "    loss_ph_imgnet = tf.placeholder(\"float\", shape=None, name='Cost_imgnet_summary')\n",
    "    loss_summ_imgnet = tf.summary.scalar('Cost_imgnet', loss_ph_imgnet)\n",
    "    \n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Initializer\n",
    "init = tf.global_variables_initializer()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(init)\n",
    "train_writer = tf.summary.FileWriter(\"./board/Cdd_{}_Trn\".format(str(cdd_n).zfill(2)), sess.graph)\n",
    "val_writer = tf.summary.FileWriter(\"./board/Cdd_{}_Val\".format(str(cdd_n).zfill(2)))\n",
    "print (\"FUNCTIONS READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32).reshape((1,1,3))\n",
    "\n",
    "# load imgnet\n",
    "# trn\n",
    "cls_name_list = ['bed','bird','cat','dog','house','tree']\n",
    "for cls_idx in range(6):\n",
    "    pth = '/users/jmy/data/image_sound/imagenet/{}_256_int8.npz'.format(cls_name_list[cls_idx])\n",
    "    l = np.load(pth)\n",
    "    globals()['{}_trn'.format(cls_name_list[cls_idx])] = l['{}_256'.format(cls_name_list[cls_idx])]\n",
    "# val\n",
    "pth = '/users/jmy/data/image_sound/imagenet/val_9550.npz'\n",
    "l = np.load(pth)\n",
    "tmp = l['imgnet_val'].reshape(9550,224,224,3).astype(np.float32) # imgnet_val_lb\n",
    "imgnet_val = np.zeros((tmp.shape),dtype=np.float32)\n",
    "for i_, img_ in enumerate(tmp):\n",
    "    new = img_ - vgg_mean\n",
    "    imgnet_val[i_] = new[:,:,::-1]\n",
    "# mnst\n",
    "pth = '/users/jmy/data/image_sound/imagenet/mnst_224_int8.npz'\n",
    "l = np.load(pth)\n",
    "mnst_trn = l['mnst_trn']\n",
    "tmp = l['mnst_val'].reshape(10000,224,224,3).astype(np.float32)\n",
    "mnst_val = np.zeros((tmp.shape),dtype=np.float32)\n",
    "for i_, img_ in enumerate(tmp):\n",
    "    new = img_ - vgg_mean\n",
    "    mnst_val[i_] = new[:,:,::-1]\n",
    "del tmp\n",
    "\n",
    "_, mt_lb = loadlocal_mnist(\n",
    "    images_path='/data/01_experiment_data/image_sound/mnist/raw/train-images-idx3-ubyte', \n",
    "    labels_path='/data/01_experiment_data/image_sound/mnist/raw/train-labels-idx1-ubyte')\n",
    "_, mv_lb = loadlocal_mnist(\n",
    "    images_path='/data/01_experiment_data/image_sound/mnist/raw/t10k-images-idx3-ubyte', \n",
    "    labels_path='/data/01_experiment_data/image_sound/mnist/raw/t10k-labels-idx1-ubyte')\n",
    "del _, l\n",
    "print(bed_trn.shape[0], bird_trn.shape[0], cat_trn.shape[0], dog_trn.shape[0], house_trn.shape[0], tree_trn.shape[0])\n",
    "print(imgnet_val.shape[0])\n",
    "print(mnst_trn.shape, mnst_val.shape, mt_lb.shape, mv_lb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labeling for 16 nodes\n",
    "# mnst\n",
    "mnst_trn_lb = np.zeros((60000,16),dtype=np.float32)\n",
    "for i,v in enumerate(mt_lb):\n",
    "    mnst_trn_lb[i,v] = 1\n",
    "mnst_val_lb = np.zeros((10000,16),dtype=np.float32)\n",
    "for i,v in enumerate(mv_lb):\n",
    "    mnst_val_lb[i,v] = 1\n",
    "\n",
    "# imgnet\n",
    "cf_list = [2690, 72641, 6180, 148544, 1611, 2663]\n",
    "for cls_idx in range(6):\n",
    "    globals()['{}_lb'.format(cls_name_list[cls_idx])] = np.zeros((cf_list[cls_idx],16),dtype=np.float32)\n",
    "    globals()['{}_lb'.format(cls_name_list[cls_idx])][:,cls_idx+10] = 1\n",
    "\n",
    "imgnet_val_lb = np.zeros((9550,16), dtype=np.float32)\n",
    "imgnet_val_lb[:150,10] = 1\n",
    "imgnet_val_lb[150:2950,11] = 1\n",
    "imgnet_val_lb[2950:3200,12] = 1\n",
    "imgnet_val_lb[3200:9150,13] = 1\n",
    "imgnet_val_lb[9150:9300,14] = 1\n",
    "imgnet_val_lb[9300:,15] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image preprocessing and augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c9 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='left-top'),iaa.Fliplr(0.5)])\n",
    "c10 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='center-top'),iaa.Fliplr(0.5)])\n",
    "c11 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='right-top'),iaa.Fliplr(0.5)])\n",
    "c12 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='left-center'),iaa.Fliplr(0.5)])\n",
    "c13 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='center'),iaa.Fliplr(0.5)])\n",
    "c14 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='right-center'),iaa.Fliplr(0.5)])\n",
    "c15 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='left-bottom'),iaa.Fliplr(0.5)])\n",
    "c16 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='center-bottom'),iaa.Fliplr(0.5)])\n",
    "c17 = iaa.Sequential([iaa.CropToFixedSize(width=224, height=224, position='right-bottom'),iaa.Fliplr(0.5)])\n",
    "c18 = iaa.Sequential([iaa.Resize({\"height\": 224, \"width\": 224}),\n",
    "                      iaa.Fliplr(0.5)])\n",
    "aug = iaa.OneOf([c9, c10, c11, c12, c13, c14, c15, c16, c17,c18])\n",
    "\n",
    "# Augmentation\n",
    "def fancy_pca(img, alpha_std=0.1):\n",
    "    orig_img = img.astype('float32').copy()\n",
    "    img = img / 255.0  # rescale to 0 to 1 range\n",
    "    img_rs = img.reshape(-1,3)\n",
    "    img_centered = img_rs - np.mean(img_rs, axis=0)\n",
    "    img_cov = np.cov(img_centered, rowvar=False)\n",
    "    eig_vals, eig_vecs = np.linalg.eigh(img_cov)\n",
    "    sort_perm = eig_vals[::-1].argsort()\n",
    "    eig_vals[::-1].sort()\n",
    "    eig_vecs = eig_vecs[:, sort_perm]\n",
    "    m1 = np.column_stack((eig_vecs))\n",
    "    m2 = np.zeros((3, 1))\n",
    "    alpha = np.random.normal(0, alpha_std)\n",
    "    m2[:, 0] = alpha * eig_vals[:]\n",
    "    add_vect = np.matrix(m1) * np.matrix(m2)\n",
    "    for idx in range(3):   # RGB\n",
    "        orig_img[..., idx] += add_vect[idx]\n",
    "    orig_img = np.clip(orig_img, 0.0, 255.0)\n",
    "    #orig_img = orig_img.astype(np.uint8) # int cast ?\n",
    "    return orig_img\n",
    "\n",
    "# imgnet\n",
    "def aug_imgnet(xs_):\n",
    "    xs_ = xs_.reshape(9666,256,256,3).copy()\n",
    "    xs_ = np.array(aug.augment_images(xs_))\n",
    "    xs2 = np.zeros((9666,224,224,3),dtype=np.float32)\n",
    "    for i_, x_ in enumerate(xs_):\n",
    "        x_ = fancy_pca(x_) - vgg_mean # fancy_pca & subtract mean\n",
    "        xs2[i_] = x_[:,:,::-1] # to bgr\n",
    "    return xs2\n",
    "\n",
    "# mnst\n",
    "def aug_mnst(xs_):\n",
    "    xs_ = xs_.reshape(16110,224,224,3).copy()\n",
    "    mnst_aug = iaa.Affine(scale=(1.0, 1.08), translate_percent=(-0.08, 0.08), rotate=(-15, 15))\n",
    "    xs_ = mnst_aug.augment_images(xs_).astype(np.float32)\n",
    "    xs2 = np.zeros((16110,224,224,3),dtype=np.float32)\n",
    "    for i_, x_ in enumerate(xs_):\n",
    "        x_ = x_ - vgg_mean\n",
    "        xs2[i_] = x_[:,:, ::-1]\n",
    "    return xs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save\n",
    "savedir = \"/users/jmy/data/nets/16_class/VGG_Base/cdd_{}/\".format(str(cdd_n).zfill(2))\n",
    "saver = tf.train.Saver(max_to_keep=500)\n",
    "save_step = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "training_epochs = 400\n",
    "batch_size = 32 # inception v3는 100epoch 32\n",
    "#display_step = 10\n",
    "ntrain = 25776\n",
    "total_batch = int(ntrain/batch_size)\n",
    "# val\n",
    "val_batch_size = 128\n",
    "tot_batch_mnst = int(10000/val_batch_size)\n",
    "tot_batch_imgnet = int(9550/val_batch_size)\n",
    "\n",
    "# for sampling\n",
    "mnstn_list = list(range(60000))\n",
    "clsn_list = [list(range(2690)),list(range(72641)),list(range(6180)),\n",
    "             list(range(148544)),list(range(1611)),list(range(2663))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Optimize\n",
    "for epoch in range(training_epochs-210):\n",
    "    epoch += 210\n",
    "    avg_cost, avg_cost_mnst, avg_cost_imgnet = 0., 0., 0.\n",
    "    avg_acc, avg_acc_mnst, avg_acc_imgnet = 0., 0., 0.\n",
    "    val_loss, val_loss_mnst, val_loss_imgnet = 0., 0., 0.\n",
    "    val_acc, val_acc_mnst, val_acc_imgnet = 0., 0., 0.\n",
    "    # random undersampling\n",
    "    mnst_rand_idx = random.sample(mnstn_list,16110) # mnst\n",
    "    mnst_udsm = mnst_trn[mnst_rand_idx]\n",
    "    imgnet_udsm = np.zeros((9666,196608),dtype='uint8') # imgnet\n",
    "    imgnet_udsm_lb = np.zeros((9666,16),dtype='float32')\n",
    "    for cls_idx in range(6):\n",
    "        cls_trn = globals()['{}_trn'.format(cls_name_list[cls_idx])]\n",
    "        cls_lb = globals()['{}_lb'.format(cls_name_list[cls_idx])]\n",
    "        cls_rand_idx = random.sample(clsn_list[cls_idx],1611)\n",
    "        imgnet_udsm[1611*cls_idx:1611*(cls_idx+1)] = cls_trn[cls_rand_idx]\n",
    "        imgnet_udsm_lb[1611*cls_idx:1611*(cls_idx+1)] = cls_lb[cls_rand_idx]\n",
    "    #label\n",
    "    y_trn = np.zeros((25776,16),dtype='float32')\n",
    "    y_trn[:16110] = mnst_trn_lb[mnst_rand_idx]\n",
    "    y_trn[16110:] = imgnet_udsm_lb\n",
    "    # augment\n",
    "    x_trn = np.zeros((25776,224,224,3),dtype='float32')\n",
    "    x_trn[:16110] = aug_mnst(mnst_udsm)\n",
    "    x_trn[16110:] = aug_imgnet(imgnet_udsm)\n",
    "    del mnst_udsm, imgnet_udsm, imgnet_udsm_lb\n",
    "    randpermlist_m = np.random.permutation(16110)\n",
    "    randpermlist_i = np.random.permutation(range(16110,25776))\n",
    "    # Iteration\n",
    "    for i in range(total_batch):\n",
    "        randidx_m = randpermlist_m[i*20:min((i+1)*20, 16110-1)]\n",
    "        randidx_i = randpermlist_i[i*12:min((i+1)*12, 9666-1)]\n",
    "        randidx = list(randidx_m)+list(randidx_i)\n",
    "        random.shuffle(randidx)\n",
    "        batch_xs = x_trn[randidx, :]\n",
    "        batch_ys = y_trn[randidx, :]\n",
    "        feeds = {x: batch_xs, y: batch_ys, is_training:True}\n",
    "        sess.run(optm, feed_dict=feeds)\n",
    "    # acc and loss \n",
    "        avg_cost += sess.run(cost, feed_dict=feeds)\n",
    "        avg_acc += sess.run(accr, feed_dict=feeds) # is_training:False?\n",
    "        # sep\n",
    "        batch_mnst_idx = np.where(np.argmax(batch_ys,axis=1)<10)[0]\n",
    "        feeds = {x: batch_xs[batch_mnst_idx], y: batch_ys[batch_mnst_idx], is_training:False}\n",
    "        avg_acc_mnst += sess.run(accr, feed_dict=feeds)\n",
    "        avg_cost_mnst += sess.run(cost, feed_dict=feeds)\n",
    "        batch_imgnet_idx = np.where(np.argmax(batch_ys,axis=1)>=10)[0]\n",
    "        feeds = {x: batch_xs[batch_imgnet_idx], y: batch_ys[batch_imgnet_idx], is_training:False}\n",
    "        avg_acc_imgnet += sess.run(accr, feed_dict=feeds)\n",
    "        avg_cost_imgnet += sess.run(cost, feed_dict=feeds)\n",
    "    avg_cost = avg_cost / total_batch\n",
    "    avg_cost_mnst = avg_cost_mnst / total_batch\n",
    "    avg_cost_imgnet = avg_cost_imgnet / total_batch\n",
    "    avg_acc = avg_acc / total_batch\n",
    "    avg_acc_mnst = avg_acc_mnst / total_batch\n",
    "    avg_acc_imgnet = avg_acc_imgnet / total_batch\n",
    "    # val\n",
    "    for i in range(tot_batch_mnst):\n",
    "        batch_xs = mnst_val[i*val_batch_size:min((i+1)*val_batch_size, 10000-1)]\n",
    "        batch_ys = mnst_val_lb[i*val_batch_size:min((i+1)*val_batch_size, 10000-1)]\n",
    "        feeds = {x: batch_xs, y: batch_ys, is_training:False}\n",
    "        val_acc_mnst += sess.run(accr, feed_dict=feeds)\n",
    "        val_loss_mnst += sess.run(cost, feed_dict=feeds)\n",
    "    val_acc_mnst = val_acc_mnst / tot_batch_mnst\n",
    "    val_loss_mnst = val_loss_mnst / tot_batch_mnst\n",
    "    for i in range(tot_batch_imgnet):\n",
    "        batch_xs = imgnet_val[i*val_batch_size:min((i+1)*val_batch_size, 9550-1)]\n",
    "        batch_ys = imgnet_val_lb[i*val_batch_size:min((i+1)*val_batch_size, 9550-1)]\n",
    "        feeds = {x: batch_xs, y: batch_ys, is_training:False}\n",
    "        val_acc_imgnet += sess.run(accr, feed_dict=feeds)\n",
    "        val_loss_imgnet += sess.run(cost, feed_dict=feeds)\n",
    "    val_acc_imgnet = val_acc_imgnet / tot_batch_imgnet\n",
    "    val_loss_imgnet = val_loss_imgnet / tot_batch_imgnet\n",
    "    val_acc = (val_acc_mnst + val_acc_imgnet)/2\n",
    "    val_loss = (val_loss_mnst + val_loss_imgnet)/2\n",
    "# write\n",
    "    # train\n",
    "    summ = sess.run(merged, feed_dict={loss_ph:avg_cost, accr_ph:avg_acc,\n",
    "                                   loss_ph_mnst:avg_cost_mnst, accr_ph_mnst:avg_acc_mnst,\n",
    "                                   loss_ph_imgnet:avg_cost_imgnet, accr_ph_imgnet:avg_acc_imgnet})\n",
    "    train_writer.add_summary(summ, epoch)\n",
    "    # val\n",
    "    summ = sess.run(merged, feed_dict={loss_ph:val_loss, accr_ph:val_acc,\n",
    "                                       loss_ph_mnst:val_loss_mnst, accr_ph_mnst:val_acc_mnst,\n",
    "                                      loss_ph_imgnet:val_loss_imgnet, accr_ph_imgnet:val_acc_imgnet})\n",
    "    val_writer.add_summary(summ, epoch)\n",
    "# Save\n",
    "    if (epoch+1) % save_step == 0 or epoch == 0:\n",
    "        savename = savedir + \"net-\" + str(epoch) + \".ckpt\"\n",
    "        saver.save(sess=sess, save_path=savename)\n",
    "        print(\"[%s] Saved\" % (savename))  \n",
    "print(\"Optimization finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
